{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7e954c",
   "metadata": {},
   "source": [
    "# Experimental Agent Creation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af5243cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "import torch\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73f9d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2bd643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBuild:\n",
    "    #Learning Parameters specifically tailored to PPO hyperparameters\n",
    "    learning_rate = 0.01 #The learning rate of the optimizer\n",
    "    gamma = 0.99 #Determines how important future rewards are to current state 0.99 is most common value\n",
    "    plot_freq = 5 #The frequency with which the model will be plotted\n",
    "    update_freq = 1 #The frequency with which the model will be updated\n",
    "    num_epochs = 5 #The number of times the model will be trained using the entire dataset\n",
    "    clip_range = 0.2 #Clipping range of the agent 0.1 - 0.3 most common clipping ranges\n",
    "    lmbda = 0.9 #Used to reduce varience in training 0.9 - 1 is the most common range of values\n",
    "    v_coef = 1 #Value function coefficient most common values 0.5 or 1\n",
    "    e_coef = 0.01 #Entropy coefficient most common values range from 0 - 0.01\n",
    "    \n",
    "    #Memory Parameters\n",
    "    mem_size = 500\n",
    "    train_if_true = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960620d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpPolicy(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(MlpPolicy, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.input_size = 8\n",
    "        self.fc1 = nn.Linear(self.input_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3_pi = nn.Linear(24, self.action_size)\n",
    "        self.fc3_v = nn.Linear(24, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        # Policy Function\n",
    "        def pi(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.fc3_pi(x)\n",
    "            return self.softmax(x)\n",
    "        \n",
    "        # Value Function\n",
    "        def val(self, x):\n",
    "            x = self.relu(self.fc1(x))\n",
    "            x = self.relu(self.fc2(x))\n",
    "            x = self.fc3_v(x)\n",
    "            return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4199141",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (418567603.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [9]\u001b[1;36m\u001b[0m\n\u001b[1;33m    if self.train\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Agent(AgentBuild):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('nasim:Tiny-v2')\n",
    "        self.action_size = self.action_space.n\n",
    "        if self.train_if_true is True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
