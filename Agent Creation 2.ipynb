{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51939f6",
   "metadata": {},
   "source": [
    "# Experimental Agent Creation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "531b2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "import torch\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f137089",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6e5c28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBuild:\n",
    "    #Learning Parameters specifically tailored to PPO hyperparameters\n",
    "    learning_rate = 0.01 #The learning rate of the optimizer\n",
    "    gamma = 0.99 #Determines how important future rewards are to current state 0.99 is most common value\n",
    "    plot_freq = 5 #The frequency with which the model will be plotted\n",
    "    update_freq = 1 #The frequency with which the model will be updated\n",
    "    num_epoch = 5 #The number of times the model will be trained using the entire dataset\n",
    "    clip_range = 0.2 #Clipping range of the agent 0.1 - 0.3 most common clipping ranges\n",
    "    lmbda = 0.9 #Used to reduce varience in training 0.9 - 1 is the most common range of values\n",
    "    v_coef = 1 #Value function coefficient most common values 0.5 or 1\n",
    "    e_coef = 0.01 #Entropy coefficient most common values range from 0 - 0.01\n",
    "    \n",
    "    #Memory Parameters\n",
    "    mem_size = 500\n",
    "    train_if_true = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2758a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpPolicy(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(MlpPolicy, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.input_size = 8\n",
    "        self.fc1 = nn.Linear(self.input_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3_pi = nn.Linear(24, self.action_size)\n",
    "        self.fc3_v = nn.Linear(24, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    # Policy Function\n",
    "    def pi(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_pi(x)\n",
    "        return self.softmax(x)\n",
    "        \n",
    "    # Value Function\n",
    "    def val(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_v(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a1c427f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASimAgent(AgentBuild):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('nasim:Tiny-v0')\n",
    "        self.action_size = self.env.action_space.n\n",
    "        if self.train_if_true:\n",
    "            self.policy = MlpPolicy(action_size = self.action_size).to(device)\n",
    "        self.opt = optim.Adam(self.policy.parameters(), lr = self.learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.opt, self.num_epoch, self.gamma)\n",
    "        self.loss = 0\n",
    "        self.memory = {\n",
    "            'state' : [],\n",
    "            'action' : [],\n",
    "            'next_state' : [],\n",
    "            'reward' : [],\n",
    "            'action_prob' : [],\n",
    "            'advantage' : [],\n",
    "            'target' : torch.FloatTensor([]),\n",
    "            'count' : 0\n",
    "        }\n",
    "    \n",
    "    # Training function for agent\n",
    "    def train(self):\n",
    "        episode = 0\n",
    "        step = 0\n",
    "        rewards = []\n",
    "        avg_reward = []\n",
    "        done = False\n",
    "        \n",
    "        # Starting a new episode\n",
    "        while not done:\n",
    "            starting_step = step\n",
    "            episode += 1\n",
    "            length_of_episode = 0\n",
    "            \n",
    "            #Setup initial state of enviornment\n",
    "            state = self.env.reset()\n",
    "            curr_state = state\n",
    "            \n",
    "            # Step in episode\n",
    "            while not done:\n",
    "                env.render_state()\n",
    "                step += 1\n",
    "                length_of_episode += 1\n",
    "                episode_reward = 0\n",
    "                \n",
    "                # Selection of next action\n",
    "                action_prob = self.policy.pi(torch.FloatTensor(curr_state).to(device))\n",
    "                action = torch.distributions.Categorical(prob_a).sample().item()\n",
    "                \n",
    "                # Performing the selected action in the current state\n",
    "                temp_state, reward, done, info = self.env.step(action)\n",
    "                next_state = temp_state\n",
    "                self.add_memory(curr_state, \n",
    "                                action, \n",
    "                                reward,\n",
    "                                new_state,\n",
    "                                prob_a[action].item())\n",
    "                curr_state = next_state\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if episode % self.update_freq == 0:\n",
    "                    for info in range(self.num_epoch):\n",
    "                        self.update_network()\n",
    "                        \n",
    "                if episode % plot_freq == 0:\n",
    "                    plot(reward_history, avg_reward)\n",
    "            self.env.close()\n",
    "            \n",
    "    # Function to plot the score against the episode number\n",
    "    def plot(reward_history, avg_reward):\n",
    "        df = pd.DataFrame({'x' : range(len(reward_history)),\n",
    "                           'Reward' : reward_history,\n",
    "                           'Average' : avg_reward})\n",
    "        plt.style.use('ggplot')\n",
    "        plt.plot(df['x'],\n",
    "                 df['Reward'], \n",
    "                 marker='',\n",
    "                 linewidth=0.7, alpha=0.9,\n",
    "                 label='Reward')\n",
    "        plt.title(\"NASim Score vs Number of Episodes Plot\", fontsize=12)\n",
    "        plt.xlabel(\"episode\", fontsize=12)\n",
    "        plt.ylabel(\"score\", fontsize=12)\n",
    "        plt.savefig('SimScores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1b1d804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    PPOAgent = NASimAgent()\n",
    "    PPOAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b80d3d",
   "metadata": {},
   "source": [
    "### Running Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "863fae05",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x56 and 8x24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [100]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_simulation\u001b[39m():\n\u001b[0;32m      2\u001b[0m     PPOAgent \u001b[38;5;241m=\u001b[39m NASimAgent()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mPPOAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [98]\u001b[0m, in \u001b[0;36mNASimAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Selection of next action\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m action_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(prob_a)\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Performing the selected action in the current state\u001b[39;00m\n",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36mMlpPolicy.pi\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpi\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3_pi(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x56 and 8x24)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
