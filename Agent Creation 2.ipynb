{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a51939f6",
   "metadata": {},
   "source": [
    "# Experimental Agent Creation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "531b2cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "import torch\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3f137089",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e5c28a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBuild:\n",
    "    #Learning Parameters specifically tailored to PPO hyperparameters\n",
    "    learning_rate = 0.01 #The learning rate of the optimizer\n",
    "    gamma = 0.99 #Determines how important future rewards are to current state 0.99 is most common value\n",
    "    plot_freq = 5 #The frequency with which the model will be plotted\n",
    "    update_freq = 1 #The frequency with which the model will be updated\n",
    "    num_epoch = 5 #The number of times the model will be trained using the entire dataset\n",
    "    clip_range = 0.2 #Clipping range of the agent 0.1 - 0.3 most common clipping ranges\n",
    "    lmbda = 0.9 #Used to reduce varience in training 0.9 - 1 is the most common range of values\n",
    "    v_coef = 1 #Value function coefficient most common values 0.5 or 1\n",
    "    e_coef = 0.01 #Entropy coefficient most common values range from 0 - 0.01\n",
    "    \n",
    "    #Memory Parameters\n",
    "    mem_size = 500\n",
    "    train_if_true = True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2758a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpPolicy(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(MlpPolicy, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.input_size = 8\n",
    "        self.fc1 = nn.Linear(self.input_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3_pi = nn.Linear(24, self.action_size)\n",
    "        self.fc3_v = nn.Linear(24, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    # Policy Function\n",
    "    def pi(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_pi(x)\n",
    "        return self.softmax(x)\n",
    "        \n",
    "    # Value Function\n",
    "    def val(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_v(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a1c427f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASimAgent(AgentBuild):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('nasim:Tiny-v0')\n",
    "        self.action_size = self.env.action_space.n\n",
    "        if self.train_if_true:\n",
    "            self.policy = MlpPolicy(action_size = self.action_size).to(device)\n",
    "        self.opt = optim.Adam(self.policy.parameters(), lr = self.learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.opt, self.num_epoch, self.gamma)\n",
    "        self.loss = 0\n",
    "        self.memory = {\n",
    "            'state' : [],\n",
    "            'action' : [],\n",
    "            'next_state' : [],\n",
    "            'reward' : [],\n",
    "            'action_prob' : [],\n",
    "            'advantage' : [],\n",
    "            'target' : torch.FloatTensor([]),\n",
    "            'count' : 0\n",
    "        }\n",
    "    \n",
    "    # Training function for agent\n",
    "    def train(self):\n",
    "        episode = 0\n",
    "        step = 0\n",
    "        rewards = []\n",
    "        avg_reward = []\n",
    "        done = False\n",
    "        \n",
    "        # Starting a new episode\n",
    "        while not done:\n",
    "            starting_step = step\n",
    "            episode += 1\n",
    "            length_of_episode = 0\n",
    "            \n",
    "            #Setup initial state of enviornment\n",
    "            state = self.env.reset()\n",
    "            curr_state = state\n",
    "            \n",
    "            # Step in episode\n",
    "            while not done:\n",
    "                env.render_state()\n",
    "                step += 1\n",
    "                length_of_episode += 1\n",
    "                episode_reward = 0\n",
    "                \n",
    "                # Selection of next action\n",
    "                action_prob = self.policy.pi(torch.FloatTensor(curr_state).to(device))\n",
    "                action = torch.distributions.Categorical(prob_a).sample().item()\n",
    "                \n",
    "                # Performing the selected action in the current state\n",
    "                temp_state, reward, done, info = self.env.step(action)\n",
    "                next_state = temp_state\n",
    "                self.add_memory(curr_state, \n",
    "                                action, \n",
    "                                reward,\n",
    "                                new_state,\n",
    "                                prob_a[action].item())\n",
    "                curr_state = next_state\n",
    "                episode_reward += reward\n",
    "                # Updating model based on episode number\n",
    "                if episode % self.update_freq == 0:\n",
    "                    for info in range(self.num_epoch):\n",
    "                        self.update_network()\n",
    "                # Updating plot based on episode number        \n",
    "                if episode % plot_freq == 0:\n",
    "                    plot(reward_history, avg_reward)\n",
    "            self.env.close()\n",
    "            \n",
    "    # Function to plot the score against the episode number\n",
    "    def plot(reward_history, avg_reward):\n",
    "        df = pd.DataFrame({'x' : range(len(reward_history)),\n",
    "                           'Reward' : reward_history,\n",
    "                           'Average' : avg_reward})\n",
    "        plt.style.use('ggplot')\n",
    "        plt.plot(df['x'],\n",
    "                 df['Reward'], \n",
    "                 marker='',\n",
    "                 linewidth=0.7, alpha=0.9,\n",
    "                 label='Reward')\n",
    "        plt.title(\"NASim Score vs Number of Episodes Plot\", fontsize=12)\n",
    "        plt.xlabel(\"episode\", fontsize=12)\n",
    "        plt.ylabel(\"score\", fontsize=12)\n",
    "        plt.savefig('SimScores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1b1d804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    PPOAgent = NASimAgent()\n",
    "    PPOAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b80d3d",
   "metadata": {},
   "source": [
    "### Running Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "863fae05",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [107]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mrun_simulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [106]\u001b[0m, in \u001b[0;36mrun_simulation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_simulation\u001b[39m():\n\u001b[0;32m      2\u001b[0m     PPOAgent \u001b[38;5;241m=\u001b[39m NASimAgent()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mPPOAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [105]\u001b[0m, in \u001b[0;36mNASimAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Step in episode\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 41\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mrender_state()\n\u001b[0;32m     42\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     43\u001b[0m     length_of_episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_simulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
