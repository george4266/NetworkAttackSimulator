{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e8ef4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, Lambda, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as kb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "import torch\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b1e06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27404f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentBuild:\n",
    "    #Learning Parameters specifically tailored to PPO hyperparameters\n",
    "    learning_rate = 0.01 #The learning rate of the optimizer\n",
    "    gamma = 0.99 #Determines how important future rewards are to current state 0.99 is most common value\n",
    "    plot_freq = 5 #The frequency with which the model will be plotted\n",
    "    update_freq = 1 #The frequency with which the model will be updated\n",
    "    num_epoch = 5 #The number of times the model will be trained using the entire dataset\n",
    "    clip_range = 0.2 #Clipping range of the agent 0.1 - 0.3 most common clipping ranges\n",
    "    lmbda = 0.9 #Used to reduce varience in training 0.9 - 1 is the most common range of values\n",
    "    v_coef = 1 #Value function coefficient most common values 0.5 or 1\n",
    "    e_coef = 0.01 #Entropy coefficient most common values range from 0 - 0.01\n",
    "    \n",
    "    #Memory Parameters\n",
    "    mem_size = 500\n",
    "    train_if_true = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32543c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MlpPolicy(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(MlpPolicy, self).__init__()\n",
    "        self.action_size = action_size\n",
    "        self.input_size = 56\n",
    "        self.fc1 = nn.Linear(self.input_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3_pi = nn.Linear(24, self.action_size)\n",
    "        self.fc3_v = nn.Linear(24, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    # Policy Function\n",
    "    def pi(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_pi(x)\n",
    "        return self.softmax(x)\n",
    "        \n",
    "    # Value Function\n",
    "    def val(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3_v(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74644240",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NASimAgent(AgentBuild):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('nasim:Tiny-v0')\n",
    "        self.action_size = self.env.action_space.n\n",
    "        if self.train_if_true:\n",
    "            self.policy = MlpPolicy(action_size = self.action_size).to(device)\n",
    "        self.opt = optim.Adam(self.policy.parameters(), lr = self.learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.opt, self.num_epoch, self.gamma)\n",
    "        self.loss = 0\n",
    "        self.memory = {\n",
    "            'state' : [],\n",
    "            'action' : [],\n",
    "            'next_state' : [],\n",
    "            'reward' : [],\n",
    "            'action_prob' : [],\n",
    "            'advantage' : [],\n",
    "            'target' : torch.FloatTensor([]),\n",
    "            'count' : 0\n",
    "        }\n",
    "        \n",
    "    def update_network(self):\n",
    "            x = 0\n",
    "    \n",
    "    # Training function for agent\n",
    "    def train(self):\n",
    "        episode = 0\n",
    "        step = 0\n",
    "        rewards = []\n",
    "        avg_reward = []\n",
    "        done = False\n",
    "        \n",
    "        # Starting a new episode\n",
    "        while not done:\n",
    "            starting_step = step\n",
    "            episode += 1\n",
    "            length_of_episode = 0\n",
    "            \n",
    "            #Setup initial state of enviornment\n",
    "            state = self.env.reset()\n",
    "            curr_state = state\n",
    "            \n",
    "            # Step in episode\n",
    "            while not done:\n",
    "                self.env.render_state()\n",
    "                step += 1\n",
    "                length_of_episode += 1\n",
    "                episode_reward = 0\n",
    "                \n",
    "                # Selection of next action\n",
    "                action_prob = self.policy.pi(torch.FloatTensor(curr_state).to(device))\n",
    "                action = torch.distributions.Categorical(action_prob).sample().item()\n",
    "                \n",
    "                # Performing the selected action in the current state\n",
    "                temp_state, reward, done, info = self.env.step(action)\n",
    "                next_state = temp_state\n",
    "                self.rem(curr_state, \n",
    "                                action, \n",
    "                                reward,\n",
    "                                next_state,\n",
    "                                action_prob[action].item())\n",
    "                curr_state = next_state\n",
    "                episode_reward += reward\n",
    "                # Updating model based on episode number\n",
    "                if episode % self.update_freq == 0:\n",
    "                    for info in range(self.num_epoch):\n",
    "                        self.update_network()\n",
    "                # Updating plot based on episode number        \n",
    "                if episode % plot_freq == 0:\n",
    "                    plot(reward_history, avg_reward)\n",
    "            self.env.close()\n",
    "            \n",
    "    # Function to plot the score against the episode number\n",
    "    def plot(reward_history, avg_reward):\n",
    "        df = pd.DataFrame({'x' : range(len(reward_history)),\n",
    "                           'Reward' : reward_history,\n",
    "                           'Average' : avg_reward})\n",
    "        plt.style.use('ggplot')\n",
    "        plt.plot(df['x'],\n",
    "                 df['Reward'], \n",
    "                 marker='',\n",
    "                 linewidth=0.7, alpha=0.9,\n",
    "                 label='Reward')\n",
    "        plt.title(\"NASim Score vs Number of Episodes Plot\", fontsize=12)\n",
    "        plt.xlabel(\"episode\", fontsize=12)\n",
    "        plt.ylabel(\"score\", fontsize=12)\n",
    "        plt.savefig('SimScores.png')\n",
    "    \n",
    "    #remember the state of agent\n",
    "    def rem(self, state, action, reward, next_state, prob):\n",
    "        if self.memory['count'] < self.mem_size:\n",
    "            self.memory['count'] += 1\n",
    "        else:\n",
    "            self.memory['state'] = self.memory['state'][1:]\n",
    "            self.memory['action'] = self.memory['action'][1:]\n",
    "            self.memory['reward'] = self.memory['reward'][1:]\n",
    "            self.memory['next_state'] = self.memory['next_state'][1:]\n",
    "            self.memory['action_prob'] = self.memory['action_prob'][1:]\n",
    "            self.memory['advantage'] = self.memory['advantage'][1:]\n",
    "\n",
    "        self.memory['state'].append(state)\n",
    "        self.memory['action'].append([action])\n",
    "        self.memory['reward'].append([reward])\n",
    "        self.memory['next_state'].append(next_state)\n",
    "        self.memory['action_prob'].append(prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da74845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    A2CAgent = NASimAgent()\n",
    "    A2CAgent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171eb0ca",
   "metadata": {},
   "source": [
    "Run Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "367b7b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:\n",
      "+---------+-------------+-----------+------------+-------+-----------------+--------+-------+------+--------+\n",
      "| Address | Compromised | Reachable | Discovered | Value | Discovery Value | Access | linux | ssh  | tomcat |\n",
      "+---------+-------------+-----------+------------+-------+-----------------+--------+-------+------+--------+\n",
      "|  (1, 0) |    False    |    True   |    True    |  0.0  |       0.0       |  0.0   |  True | True |  True  |\n",
      "|  (2, 0) |    False    |   False   |   False    | 100.0 |       0.0       |  0.0   |  True | True |  True  |\n",
      "|  (3, 0) |    False    |   False   |   False    | 100.0 |       0.0       |  0.0   |  True | True |  True  |\n",
      "+---------+-------------+-----------+------------+-------+-----------------+--------+-------+------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tj/107lbdtn5tg9w338mn510w480000gn/T/ipykernel_45363/1388148311.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(x)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plot_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tj/107lbdtn5tg9w338mn510w480000gn/T/ipykernel_45363/3309409040.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tj/107lbdtn5tg9w338mn510w480000gn/T/ipykernel_45363/618576739.py\u001b[0m in \u001b[0;36mrun_simulation\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mA2CAgent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNASimAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mA2CAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tj/107lbdtn5tg9w338mn510w480000gn/T/ipykernel_45363/206871026.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0;31m# Updating plot based on episode number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mplot_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                     \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_freq' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af989743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
